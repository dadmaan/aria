{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Exploratory Data Analysis: Music Generation Datasets\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a comprehensive exploratory data analysis of the processed datasets used in the multi-agent reinforcement learning framework for symbolic music generation. The analysis examines three key datasets:\n",
    "\n",
    "1. **ComMU Bass Dataset**: Curated bass tracks from the ComMU dataset with rich metadata\n",
    "2. **Bass Loops Dataset**: Collection of bass loops primarily from dance and jazz genres\n",
    "3. **Combined Dataset**: Merged dataset combining both sources\n",
    "\n",
    "### Analysis Goals\n",
    "\n",
    "This EDA focuses on understanding:\n",
    "- Musical feature distributions and characteristics\n",
    "- Genre and style patterns across datasets\n",
    "- Harmonic structures through chord progression analysis\n",
    "- Temporal and rhythmic characteristics\n",
    "- Dataset composition and split distributions\n",
    "- Feature relationships and correlations\n",
    "\n",
    "### Context\n",
    "\n",
    "This analysis supports a multi-agent RL system that combines:\n",
    "- **Perceiving Agent (GHSOM)**: Clusters musical features and discovers structural motifs\n",
    "- **Generative Agent (DQN with LSTM)**: Selects musical elements to build sequences\n",
    "- **Human Agent**: Provides feedback for reward shaping and adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths with fallbacks\n",
    "candidate_dirs = [\n",
    "    Path('/workspace/data/subset'),\n",
    "    Path.cwd() / 'data' / 'subset',\n",
    "    Path.cwd().parent / 'data' / 'subset',\n",
    "    Path.cwd().parent.parent / 'data' / 'subset',\n",
    "]\n",
    "DATA_DIR = next((p for p in candidate_dirs if (p / 'combined_metadata_clean.csv').exists()), None)\n",
    "if DATA_DIR is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate combined_metadata_clean.csv in any expected data directory.\"\n",
    "    )\n",
    "DATA_DIR = DATA_DIR.resolve()\n",
    "print(f\"Using data directory: {DATA_DIR}\")\n",
    "\n",
    "# Load datasets\n",
    "df_combined = pd.read_csv(DATA_DIR / 'combined_metadata_clean.csv')\n",
    "df_commu = pd.read_csv(DATA_DIR / 'commu' / 'bass' / 'metadata_clean.csv')\n",
    "df_bass_loops = pd.read_csv(DATA_DIR / 'LM_bass_loops_matched' / 'metadata_clean.csv')\n",
    "\n",
    "print(f\"Combined Dataset: {len(df_combined):,} samples\")\n",
    "print(f\"ComMU Bass Dataset: {len(df_commu):,} samples\")\n",
    "print(f\"Bass Loops Dataset: {len(df_bass_loops):,} samples\")\n",
    "print(f\"\\nTotal unique samples: {len(df_combined):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about datasets\n",
    "print(\"=\" * 80)\n",
    "print(\"COMBINED DATASET INFO\")\n",
    "print(\"=\" * 80)\n",
    "print(df_combined.info())\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\" * 80)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Composition Analysis\n",
    "\n",
    "Understanding the composition and structure of our datasets is crucial for:\n",
    "- Ensuring balanced training/validation/test splits\n",
    "- Understanding data source distributions\n",
    "- Identifying potential biases in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset sources\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Source distribution\n",
    "if 'source' in df_combined.columns:\n",
    "    source_counts = df_combined['source'].value_counts()\n",
    "    axes[0].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0].set_title('Dataset Source Distribution', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'Source column not available', ha='center', va='center')\n",
    "    axes[0].set_title('Dataset Source Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Split distribution\n",
    "split_counts = df_combined['split'].value_counts()\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "axes[1].bar(split_counts.index, split_counts.values, color=colors)\n",
    "axes[1].set_title('Train/Validation/Test Split', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Number of Samples')\n",
    "axes[1].set_xlabel('Split')\n",
    "for i, v in enumerate(split_counts.values):\n",
    "    axes[1].text(i, v + 200, f'{v:,}\\n({v/len(df_combined)*100:.1f}%)', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Split distribution by source\n",
    "if 'source' in df_combined.columns:\n",
    "    split_source = pd.crosstab(df_combined['split'], df_combined['source'])\n",
    "    split_source.plot(kind='bar', stacked=True, ax=axes[2], color=['#9b59b6', '#f39c12'])\n",
    "    axes[2].set_title('Split Distribution by Source', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_ylabel('Number of Samples')\n",
    "    axes[2].set_xlabel('Split')\n",
    "    axes[2].legend(title='Source')\n",
    "    axes[2].tick_params(axis='x', rotation=0)\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, 'Source column not available', ha='center', va='center')\n",
    "    axes[2].set_title('Split Distribution by Source', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\nDetailed Split Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "for split in ['train', 'val', 'test']:\n",
    "    count = len(df_combined[df_combined['split'] == split])\n",
    "    pct = count / len(df_combined) * 100\n",
    "    print(f\"{split.capitalize():10s}: {count:6,} samples ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Genre and Style Analysis\n",
    "\n",
    "Genre distribution is critical for understanding:\n",
    "- The stylistic diversity of the training data\n",
    "- Potential genre biases in the model\n",
    "- Coverage of different musical styles for generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genre analysis\n",
    "def parse_genres(df):\n",
    "    \"\"\"Parse genre field which may contain multiple genres separated by |.\"\"\"\n",
    "    all_genres = []\n",
    "    for genre in df['genre'].dropna():\n",
    "        if '|' in str(genre):\n",
    "            all_genres.extend(str(genre).split('|'))\n",
    "        else:\n",
    "            all_genres.append(str(genre))\n",
    "    return pd.Series(all_genres).value_counts()\n",
    "\n",
    "genre_counts = parse_genres(df_combined)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Genre distribution bar chart\n",
    "top_genres = genre_counts.head(15)\n",
    "axes[0].barh(range(len(top_genres)), top_genres.values, color=sns.color_palette('viridis', len(top_genres)))\n",
    "axes[0].set_yticks(range(len(top_genres)))\n",
    "axes[0].set_yticklabels(top_genres.index)\n",
    "axes[0].set_xlabel('Number of Tracks')\n",
    "axes[0].set_title('Top 15 Genres in Dataset', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "for i, v in enumerate(top_genres.values):\n",
    "    axes[0].text(v + 50, i, f'{v:,}', va='center')\n",
    "\n",
    "# Genre distribution by source\n",
    "if 'source' in df_combined.columns:\n",
    "    genre_source_data = []\n",
    "    for source in df_combined['source'].unique():\n",
    "        source_df = df_combined[df_combined['source'] == source]\n",
    "        source_genres = parse_genres(source_df)\n",
    "        genre_source_data.append(source_genres.head(10))\n",
    "    \n",
    "    # Combine for comparison\n",
    "    genre_comparison = pd.DataFrame(genre_source_data).T.fillna(0)\n",
    "    genre_comparison.columns = df_combined['source'].unique()\n",
    "    genre_comparison.plot(kind='bar', ax=axes[1], color=['#9b59b6', '#f39c12'])\n",
    "    axes[1].set_title('Top Genres by Source', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Number of Tracks')\n",
    "    axes[1].set_xlabel('Genre')\n",
    "    axes[1].legend(title='Source')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal unique genres: {len(genre_counts)}\")\n",
    "print(f\"\\nTop 10 Genres:\")\n",
    "print(\"=\" * 50)\n",
    "for genre, count in genre_counts.head(10).items():\n",
    "    print(f\"{genre:20s}: {count:6,} ({count/len(df_combined)*100:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Musical Feature Analysis\n",
    "\n",
    "### 4.1 Tempo (BPM) Distribution\n",
    "\n",
    "Tempo is a fundamental characteristic affecting:\n",
    "- Energy and feel of generated music\n",
    "- Rhythmic complexity\n",
    "- Genre characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPM analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Overall BPM distribution\n",
    "axes[0, 0].hist(df_combined['bpm'].dropna(), bins=50, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(df_combined['bpm'].median(), color='red', linestyle='--', linewidth=2, label=f'Median: {df_combined[\"bpm\"].median():.1f}')\n",
    "axes[0, 0].axvline(df_combined['bpm'].mean(), color='orange', linestyle='--', linewidth=2, label=f'Mean: {df_combined[\"bpm\"].mean():.1f}')\n",
    "axes[0, 0].set_xlabel('BPM (Beats Per Minute)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('BPM Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# BPM by source\n",
    "if 'source' in df_combined.columns:\n",
    "    sources = df_combined['source'].unique()\n",
    "    bpm_by_source = [df_combined[df_combined['source'] == src]['bpm'].dropna() for src in sources]\n",
    "    axes[0, 1].violinplot(bpm_by_source, positions=range(len(sources)), showmeans=True, showmedians=True)\n",
    "    axes[0, 1].set_xticks(range(len(sources)))\n",
    "    axes[0, 1].set_xticklabels(sources)\n",
    "    axes[0, 1].set_ylabel('BPM')\n",
    "    axes[0, 1].set_title('BPM Distribution by Source', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# BPM by genre (top genres)\n",
    "top_genre_list = genre_counts.head(8).index.tolist()\n",
    "bpm_genre_data = []\n",
    "genre_labels = []\n",
    "for genre in top_genre_list:\n",
    "    mask = df_combined['genre'].str.contains(genre, na=False, case=False)\n",
    "    genre_bpm = df_combined[mask]['bpm'].dropna()\n",
    "    if len(genre_bpm) > 0:\n",
    "        bpm_genre_data.append(genre_bpm)\n",
    "        genre_labels.append(genre)\n",
    "\n",
    "bp = axes[1, 0].boxplot(bpm_genre_data, labels=genre_labels, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], sns.color_palette('Set2', len(genre_labels))):\n",
    "    patch.set_facecolor(color)\n",
    "axes[1, 0].set_xlabel('Genre')\n",
    "axes[1, 0].set_ylabel('BPM')\n",
    "axes[1, 0].set_title('BPM Distribution by Genre', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# BPM statistics by split\n",
    "split_stats = df_combined.groupby('split')['bpm'].agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "x = np.arange(len(split_stats.index))\n",
    "width = 0.35\n",
    "axes[1, 1].bar(x - width/2, split_stats['mean'], width, label='Mean', color='#3498db')\n",
    "axes[1, 1].bar(x + width/2, split_stats['median'], width, label='Median', color='#2ecc71')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(split_stats.index)\n",
    "axes[1, 1].set_ylabel('BPM')\n",
    "axes[1, 1].set_title('BPM Statistics by Split', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nBPM Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(df_combined['bpm'].describe())\n",
    "print(f\"\\nBPM Range: {df_combined['bpm'].min():.1f} - {df_combined['bpm'].max():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Pitch Range Analysis\n",
    "\n",
    "Pitch range indicates:\n",
    "- Melodic complexity and span\n",
    "- Instrument capabilities\n",
    "- Musical expressiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pitch_range to numeric if it's categorical\n",
    "pitch_range_mapping = {\n",
    "    'low': 1,\n",
    "    'mid_low': 2,\n",
    "    'mid': 3,\n",
    "    'mid_high': 4,\n",
    "    'high': 5\n",
    "}\n",
    "\n",
    "df_combined['pitch_range_numeric'] = df_combined['pitch_range'].map(pitch_range_mapping)\n",
    "\n",
    "# Also handle numeric pitch ranges if they exist\n",
    "df_combined['pitch_range_value'] = pd.to_numeric(df_combined['pitch_range'], errors='coerce')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Pitch range distribution (categorical)\n",
    "if df_combined['pitch_range_numeric'].notna().any():\n",
    "    pitch_counts = (\n",
    "        df_combined['pitch_range']\n",
    "        .fillna('unknown')\n",
    "        .astype(str)\n",
    "        .value_counts()\n",
    "        .sort_index(key=lambda idx: idx.astype(str))\n",
    "    )\n",
    "    axes[0, 0].bar(range(len(pitch_counts)), pitch_counts.values, \n",
    "                   color=sns.color_palette('coolwarm', len(pitch_counts)))\n",
    "    axes[0, 0].set_xticks(range(len(pitch_counts)))\n",
    "    axes[0, 0].set_xticklabels(pitch_counts.index, rotation=45)\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Pitch Range Distribution (Categorical)', fontsize=14, fontweight='bold')\n",
    "    for i, v in enumerate(pitch_counts.values):\n",
    "        axes[0, 0].text(i, v + 50, f'{v:,}', ha='center', va='bottom')\n",
    "\n",
    "# Pitch range distribution (numeric)\n",
    "if df_combined['pitch_range_value'].notna().any():\n",
    "    axes[0, 1].hist(df_combined['pitch_range_value'].dropna(), bins=30, \n",
    "                    color='#9b59b6', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Pitch Range (Semitones)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Pitch Range Distribution (Numeric)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Pitch range by source\n",
    "if 'source' in df_combined.columns and df_combined['pitch_range_numeric'].notna().any():\n",
    "    pitch_source = pd.crosstab(df_combined['pitch_range'].fillna('unknown'), df_combined['source'])\n",
    "    pitch_source.plot(kind='bar', ax=axes[1, 0], color=['#9b59b6', '#f39c12'])\n",
    "    axes[1, 0].set_title('Pitch Range by Source', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Number of Tracks')\n",
    "    axes[1, 0].set_xlabel('Pitch Range')\n",
    "    axes[1, 0].legend(title='Source')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pitch range vs BPM scatter\n",
    "if df_combined['pitch_range_numeric'].notna().any():\n",
    "    scatter_data = df_combined[df_combined['pitch_range_numeric'].notna()]\n",
    "    axes[1, 1].scatter(scatter_data['bpm'], scatter_data['pitch_range_numeric'], \n",
    "                       alpha=0.5, c=scatter_data['pitch_range_numeric'], \n",
    "                       cmap='coolwarm', s=20)\n",
    "    axes[1, 1].set_xlabel('BPM')\n",
    "    axes[1, 1].set_ylabel('Pitch Range (Encoded)')\n",
    "    axes[1, 1].set_title('Pitch Range vs BPM', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nPitch Range Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "pitch_distribution = (\n",
    "    df_combined['pitch_range']\n",
    "    .fillna('unknown')\n",
    "    .astype(str)\n",
    "    .value_counts()\n",
    "    .sort_index(key=lambda idx: idx.astype(str))\n",
    ")\n",
    "print(pitch_distribution)\n",
    "if df_combined['pitch_range_value'].notna().any():\n",
    "    print(\"\\nNumeric Pitch Range Statistics:\")\n",
    "    print(df_combined['pitch_range_value'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Time Signature Analysis\n",
    "\n",
    "Time signatures define:\n",
    "- Rhythmic framework\n",
    "- Beat patterns\n",
    "- Musical complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time signature analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Time signature distribution\n",
    "time_sig_counts = df_combined['time_signature'].value_counts()\n",
    "axes[0].pie(time_sig_counts.values, labels=time_sig_counts.index, autopct='%1.1f%%', \n",
    "            startangle=90, colors=sns.color_palette('Set3', len(time_sig_counts)))\n",
    "axes[0].set_title('Time Signature Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Time signature by genre\n",
    "if len(genre_labels) > 0:\n",
    "    time_sig_genre_data = []\n",
    "    for genre in genre_labels[:5]:  # Top 5 genres\n",
    "        mask = df_combined['genre'].str.contains(genre, na=False, case=False)\n",
    "        genre_time_sigs = df_combined[mask]['time_signature'].value_counts()\n",
    "        time_sig_genre_data.append(genre_time_sigs)\n",
    "    \n",
    "    time_sig_df = pd.DataFrame(time_sig_genre_data, index=genre_labels[:5]).fillna(0).T\n",
    "    time_sig_df.plot(kind='bar', ax=axes[1], stacked=True)\n",
    "    axes[1].set_title('Time Signature by Top Genres', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Number of Tracks')\n",
    "    axes[1].set_xlabel('Time Signature')\n",
    "    axes[1].legend(title='Genre', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Time signature vs BPM\n",
    "time_sig_bpm = df_combined.groupby('time_signature')['bpm'].agg(['mean', 'std'])\n",
    "x = np.arange(len(time_sig_bpm.index))\n",
    "axes[2].bar(x, time_sig_bpm['mean'], yerr=time_sig_bpm['std'], \n",
    "            capsize=5, color='#3498db', alpha=0.7)\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(time_sig_bpm.index)\n",
    "axes[2].set_ylabel('Average BPM')\n",
    "axes[2].set_xlabel('Time Signature')\n",
    "axes[2].set_title('Average BPM by Time Signature', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTime Signature Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "for ts, count in time_sig_counts.items():\n",
    "    print(f\"{ts:10s}: {count:6,} ({count/len(df_combined)*100:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Number of Measures\n",
    "\n",
    "The number of measures indicates:\n",
    "- Sequence length\n",
    "- Structural complexity\n",
    "- Learning challenges for the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measures analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Measures distribution\n",
    "axes[0].hist(df_combined['num_measures'].dropna(), bins=20, color='#2ecc71', \n",
    "             edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df_combined['num_measures'].median(), color='red', \n",
    "                linestyle='--', linewidth=2, label=f'Median: {df_combined[\"num_measures\"].median():.0f}')\n",
    "axes[0].axvline(df_combined['num_measures'].mean(), color='orange', \n",
    "                linestyle='--', linewidth=2, label=f'Mean: {df_combined[\"num_measures\"].mean():.1f}')\n",
    "axes[0].set_xlabel('Number of Measures')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Number of Measures Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Measures by source\n",
    "if 'source' in df_combined.columns:\n",
    "    sources = df_combined['source'].unique()\n",
    "    measures_by_source = [df_combined[df_combined['source'] == src]['num_measures'].dropna() \n",
    "                          for src in sources]\n",
    "    bp = axes[1].boxplot(measures_by_source, labels=sources, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], sns.color_palette('Set2', len(sources))):\n",
    "        patch.set_facecolor(color)\n",
    "    axes[1].set_ylabel('Number of Measures')\n",
    "    axes[1].set_title('Measures Distribution by Source', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Measures vs BPM\n",
    "axes[2].scatter(df_combined['num_measures'], df_combined['bpm'], \n",
    "                alpha=0.3, s=20, c='#e74c3c')\n",
    "axes[2].set_xlabel('Number of Measures')\n",
    "axes[2].set_ylabel('BPM')\n",
    "axes[2].set_title('Number of Measures vs BPM', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "corr = df_combined[['num_measures', 'bpm']].corr().iloc[0, 1]\n",
    "axes[2].text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "             transform=axes[2].transAxes, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNumber of Measures Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(df_combined['num_measures'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Velocity Analysis\n",
    "\n",
    "Velocity (MIDI dynamics) reflects:\n",
    "- Dynamic range\n",
    "- Expressive potential\n",
    "- Performance characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Velocity analysis\n",
    "# Filter out rows where velocity data is available\n",
    "df_velocity = df_combined[(df_combined['min_velocity'].notna()) & \n",
    "                          (df_combined['max_velocity'].notna())].copy()\n",
    "\n",
    "if len(df_velocity) > 0:\n",
    "    df_velocity['velocity_range'] = df_velocity['max_velocity'] - df_velocity['min_velocity']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # Min velocity distribution\n",
    "    axes[0, 0].hist(df_velocity['min_velocity'], bins=30, color='#3498db', \n",
    "                    edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Minimum Velocity')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Minimum Velocity Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Max velocity distribution\n",
    "    axes[0, 1].hist(df_velocity['max_velocity'], bins=30, color='#e74c3c', \n",
    "                    edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Maximum Velocity')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Maximum Velocity Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Velocity range distribution\n",
    "    axes[1, 0].hist(df_velocity['velocity_range'], bins=30, color='#2ecc71', \n",
    "                    edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Velocity Range (Max - Min)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Velocity Range Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Min vs Max velocity scatter\n",
    "    axes[1, 1].scatter(df_velocity['min_velocity'], df_velocity['max_velocity'], \n",
    "                       alpha=0.5, s=20, c=df_velocity['velocity_range'], \n",
    "                       cmap='viridis')\n",
    "    axes[1, 1].plot([0, 127], [0, 127], 'r--', alpha=0.5, label='y=x')\n",
    "    axes[1, 1].set_xlabel('Minimum Velocity')\n",
    "    axes[1, 1].set_ylabel('Maximum Velocity')\n",
    "    axes[1, 1].set_title('Min vs Max Velocity', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    cbar = plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1])\n",
    "    cbar.set_label('Velocity Range')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nVelocity Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nMinimum Velocity:\")\n",
    "    print(df_velocity['min_velocity'].describe())\n",
    "    print(\"\\nMaximum Velocity:\")\n",
    "    print(df_velocity['max_velocity'].describe())\n",
    "    print(\"\\nVelocity Range:\")\n",
    "    print(df_velocity['velocity_range'].describe())\n",
    "else:\n",
    "    print(\"\\nNo velocity data available in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Harmonic Analysis: Chord Progressions\n",
    "\n",
    "Chord progressions are crucial for:\n",
    "- Understanding harmonic patterns\n",
    "- Identifying common musical structures\n",
    "- Training the perceiving agent to recognize motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chord progression analysis\n",
    "def parse_chord_progressions(df, sample_size=None):\n",
    "    \"\"\"Extract individual chords from chord progression field.\"\"\"\n",
    "    all_chords = []\n",
    "    \n",
    "    # Sample if dataset is large\n",
    "    df_sample = df.sample(n=min(sample_size or len(df), len(df)), random_state=42)\n",
    "    \n",
    "    for prog in df_sample['chord_progressions'].dropna():\n",
    "        try:\n",
    "            # Parse the string representation of list\n",
    "            if isinstance(prog, str) and prog != '[]':\n",
    "                chord_list = ast.literal_eval(prog)\n",
    "                if isinstance(chord_list, list) and len(chord_list) > 0:\n",
    "                    # Flatten nested lists\n",
    "                    for item in chord_list:\n",
    "                        if isinstance(item, list):\n",
    "                            all_chords.extend(item)\n",
    "                        else:\n",
    "                            all_chords.append(item)\n",
    "        except (ValueError, SyntaxError):\n",
    "            continue\n",
    "    \n",
    "    return pd.Series(all_chords).value_counts()\n",
    "\n",
    "# Get chord statistics\n",
    "chord_counts = parse_chord_progressions(df_combined, sample_size=10000)\n",
    "\n",
    "if len(chord_counts) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # Top chords\n",
    "    top_chords = chord_counts.head(20)\n",
    "    axes[0, 0].barh(range(len(top_chords)), top_chords.values, \n",
    "                    color=sns.color_palette('viridis', len(top_chords)))\n",
    "    axes[0, 0].set_yticks(range(len(top_chords)))\n",
    "    axes[0, 0].set_yticklabels(top_chords.index)\n",
    "    axes[0, 0].set_xlabel('Frequency')\n",
    "    axes[0, 0].set_title('Top 20 Most Common Chords', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].invert_yaxis()\n",
    "    for i, v in enumerate(top_chords.values):\n",
    "        axes[0, 0].text(v + 10, i, f'{v:,}', va='center')\n",
    "    \n",
    "    # Chord frequency distribution\n",
    "    axes[0, 1].hist(chord_counts.values, bins=50, color='#9b59b6', \n",
    "                    edgecolor='black', alpha=0.7, log=True)\n",
    "    axes[0, 1].set_xlabel('Chord Frequency')\n",
    "    axes[0, 1].set_ylabel('Number of Chords (log scale)')\n",
    "    axes[0, 1].set_title('Chord Frequency Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Chord type analysis (major, minor, 7th, etc.)\n",
    "    chord_types = {\n",
    "        'Major': sum(1 for c in chord_counts.index if 'm' not in c.lower() and '7' not in c),\n",
    "        'Minor': sum(1 for c in chord_counts.index if 'm' in c.lower() and '7' not in c),\n",
    "        'Seventh': sum(1 for c in chord_counts.index if '7' in c),\n",
    "        'Diminished': sum(1 for c in chord_counts.index if 'dim' in c.lower()),\n",
    "        'Augmented': sum(1 for c in chord_counts.index if '+' in c or 'aug' in c.lower()),\n",
    "        'Suspended': sum(1 for c in chord_counts.index if 'sus' in c.lower()),\n",
    "    }\n",
    "    \n",
    "    axes[1, 0].pie(chord_types.values(), labels=chord_types.keys(), autopct='%1.1f%%', \n",
    "                   startangle=90, colors=sns.color_palette('Set2', len(chord_types)))\n",
    "    axes[1, 0].set_title('Chord Type Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Root note distribution\n",
    "    root_notes = []\n",
    "    for chord in chord_counts.index:\n",
    "        # Extract root note (first letter(s) before modifiers)\n",
    "        root = chord.split('m')[0].split('7')[0].split('sus')[0].split('dim')[0].split('+')[0]\n",
    "        root_notes.append(root)\n",
    "    \n",
    "    root_counts = pd.Series(root_notes).value_counts().head(12)\n",
    "    axes[1, 1].bar(range(len(root_counts)), root_counts.values, \n",
    "                   color=sns.color_palette('husl', len(root_counts)))\n",
    "    axes[1, 1].set_xticks(range(len(root_counts)))\n",
    "    axes[1, 1].set_xticklabels(root_counts.index, rotation=45)\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_xlabel('Root Note')\n",
    "    axes[1, 1].set_title('Root Note Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nChord Progression Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total unique chords: {len(chord_counts):,}\")\n",
    "    print(f\"Total chord occurrences: {chord_counts.sum():,}\")\n",
    "    print(f\"\\nTop 10 Most Common Chords:\")\n",
    "    for i, (chord, count) in enumerate(chord_counts.head(10).items(), 1):\n",
    "        print(f\"{i:2d}. {chord:15s}: {count:6,}\")\n",
    "else:\n",
    "    print(\"\\nNo chord progression data available or could not parse chord progressions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Instrument and Audio Key Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instrument and key analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Instrument distribution\n",
    "inst_counts = df_combined['inst'].value_counts().head(15)\n",
    "axes[0, 0].barh(range(len(inst_counts)), inst_counts.values, \n",
    "                color=sns.color_palette('tab20', len(inst_counts)))\n",
    "axes[0, 0].set_yticks(range(len(inst_counts)))\n",
    "axes[0, 0].set_yticklabels(inst_counts.index)\n",
    "axes[0, 0].set_xlabel('Frequency')\n",
    "axes[0, 0].set_title('Top 15 Instruments', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].invert_yaxis()\n",
    "for i, v in enumerate(inst_counts.values):\n",
    "    axes[0, 0].text(v + 50, i, f'{v:,}', va='center')\n",
    "\n",
    "# Audio key distribution\n",
    "key_counts = df_combined['audio_key'].value_counts().head(15)\n",
    "axes[0, 1].bar(range(len(key_counts)), key_counts.values, \n",
    "               color=sns.color_palette('rainbow', len(key_counts)))\n",
    "axes[0, 1].set_xticks(range(len(key_counts)))\n",
    "axes[0, 1].set_xticklabels(key_counts.index, rotation=45, ha='right')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Top 15 Audio Keys', fontsize=14, fontweight='bold')\n",
    "for i, v in enumerate(key_counts.values):\n",
    "    axes[0, 1].text(i, v + 50, f'{v:,}', ha='center', va='bottom', rotation=0)\n",
    "\n",
    "# Track role distribution\n",
    "role_counts = df_combined['track_role'].value_counts()\n",
    "axes[1, 0].pie(role_counts.values, labels=role_counts.index, autopct='%1.1f%%', \n",
    "               startangle=90, colors=sns.color_palette('pastel', len(role_counts)))\n",
    "axes[1, 0].set_title('Track Role Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Sample rhythm distribution\n",
    "rhythm_counts = df_combined['sample_rhythm'].value_counts().head(10)\n",
    "axes[1, 1].bar(range(len(rhythm_counts)), rhythm_counts.values, \n",
    "               color=sns.color_palette('muted', len(rhythm_counts)))\n",
    "axes[1, 1].set_xticks(range(len(rhythm_counts)))\n",
    "axes[1, 1].set_xticklabels(rhythm_counts.index, rotation=45, ha='right')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Top 10 Sample Rhythms', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInstrument Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total unique instruments: {df_combined['inst'].nunique()}\")\n",
    "print(f\"\\nTop 10 Instruments:\")\n",
    "for inst, count in inst_counts.head(10).items():\n",
    "    print(f\"{inst:30s}: {count:6,} ({count/len(df_combined)*100:5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Audio Key Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total unique keys: {df_combined['audio_key'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Correlation Analysis\n",
    "\n",
    "Understanding feature correlations helps:\n",
    "- Identify redundant features\n",
    "- Understand feature relationships\n",
    "- Guide feature engineering for the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation analysis\n",
    "# Select numeric features for correlation\n",
    "numeric_features = ['bpm', 'num_measures']\n",
    "\n",
    "# Add pitch_range_numeric if available\n",
    "if 'pitch_range_numeric' in df_combined.columns and df_combined['pitch_range_numeric'].notna().any():\n",
    "    numeric_features.append('pitch_range_numeric')\n",
    "\n",
    "# Add velocity features if available\n",
    "if 'min_velocity' in df_combined.columns and df_combined['min_velocity'].notna().any():\n",
    "    numeric_features.extend(['min_velocity', 'max_velocity'])\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_combined[numeric_features].corr()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Correlation heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, ax=axes[0], \n",
    "            cbar_kws={\"shrink\": 0.8}, fmt='.3f')\n",
    "axes[0].set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Feature pairplot (sample for performance)\n",
    "clean_numeric = df_combined[numeric_features].dropna()\n",
    "sample_size = min(1000, len(clean_numeric))\n",
    "sample_df = clean_numeric.sample(n=sample_size, random_state=42) if sample_size > 0 else clean_numeric\n",
    "axes[1].axis('off')\n",
    "axes[1].text(0.5, 0.5, 'See detailed pairplot below', \n",
    "             ha='center', va='center', fontsize=12, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create pairplot for detailed view\n",
    "if len(clean_numeric) > 0:\n",
    "    print(\"\\nGenerating detailed pairplot (this may take a moment)...\\n\")\n",
    "    pairplot_sample = clean_numeric.sample(\n",
    "        n=min(500, len(clean_numeric)), random_state=42\n",
    "    )\n",
    "    sns.pairplot(pairplot_sample, diag_kind='kde', plot_kws={'alpha': 0.6})\n",
    "    plt.suptitle('Feature Pairplot (Sample)', y=1.01, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nInsufficient numeric data for pairplot.\")\n",
    "\n",
    "print(\"\\nFeature Correlation Matrix:\")\n",
    "print(\"=\" * 50)\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "print(\"Data Quality Report\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Missing values analysis\n",
    "missing_values = df_combined.isnull().sum()\n",
    "missing_pct = (missing_values / len(df_combined)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_pct\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(\"-\" * 80)\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Missing values bar chart\n",
    "missing_features = missing_df[missing_df['Missing Count'] > 0]\n",
    "if len(missing_features) > 0:\n",
    "    axes[0].barh(range(len(missing_features)), missing_features['Percentage'].values, \n",
    "                 color='#e74c3c')\n",
    "    axes[0].set_yticks(range(len(missing_features)))\n",
    "    axes[0].set_yticklabels(missing_features.index)\n",
    "    axes[0].set_xlabel('Percentage Missing (%)')\n",
    "    axes[0].set_title('Missing Values by Feature', fontsize=14, fontweight='bold')\n",
    "    axes[0].invert_yaxis()\n",
    "    for i, v in enumerate(missing_features['Percentage'].values):\n",
    "        axes[0].text(v + 0.5, i, f'{v:.1f}%', va='center')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No missing values found!', \n",
    "                 ha='center', va='center', fontsize=14, style='italic')\n",
    "    axes[0].set_title('Missing Values by Feature', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Data completeness by split\n",
    "completeness_by_split = []\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_df = df_combined[df_combined['split'] == split]\n",
    "    completeness = (1 - split_df.isnull().sum() / len(split_df)) * 100\n",
    "    completeness_by_split.append(completeness.mean())\n",
    "\n",
    "axes[1].bar(['Train', 'Val', 'Test'], completeness_by_split, \n",
    "            color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "axes[1].set_ylabel('Completeness (%)')\n",
    "axes[1].set_title('Data Completeness by Split', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim([0, 105])\n",
    "for i, v in enumerate(completeness_by_split):\n",
    "    axes[1].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Duplicate analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Duplicate Analysis:\")\n",
    "print(\"-\" * 80)\n",
    "duplicates = df_combined.duplicated(subset=['id']).sum()\n",
    "print(f\"Duplicate IDs: {duplicates}\")\n",
    "print(f\"Unique samples: {df_combined['id'].nunique():,}\")\n",
    "\n",
    "# Data type consistency\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Data Types:\")\n",
    "print(\"-\" * 80)\n",
    "print(df_combined.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE DATASET SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = {\n",
    "    'Dataset Size': {\n",
    "        'Total Samples': len(df_combined),\n",
    "        'Training Samples': len(df_combined[df_combined['split'] == 'train']),\n",
    "        'Validation Samples': len(df_combined[df_combined['split'] == 'val']),\n",
    "        'Test Samples': len(df_combined[df_combined['split'] == 'test']),\n",
    "    },\n",
    "    'Musical Characteristics': {\n",
    "        'BPM Range': f\"{df_combined['bpm'].min():.1f} - {df_combined['bpm'].max():.1f}\",\n",
    "        'Average BPM': f\"{df_combined['bpm'].mean():.1f}\",\n",
    "        'Unique Genres': df_combined['genre'].nunique(),\n",
    "        'Unique Instruments': df_combined['inst'].nunique(),\n",
    "        'Most Common Time Signature': df_combined['time_signature'].mode()[0] if len(df_combined) > 0 else 'N/A',\n",
    "    },\n",
    "    'Complexity Metrics': {\n",
    "        'Average Measures': f\"{df_combined['num_measures'].mean():.1f}\",\n",
    "        'Measures Range': f\"{df_combined['num_measures'].min():.0f} - {df_combined['num_measures'].max():.0f}\",\n",
    "    }\n",
    "}\n",
    "\n",
    "if 'source' in df_combined.columns:\n",
    "    summary['Data Sources'] = dict(df_combined['source'].value_counts())\n",
    "\n",
    "if len(chord_counts) > 0:\n",
    "    summary['Harmonic Characteristics'] = {\n",
    "        'Unique Chords': len(chord_counts),\n",
    "        'Most Common Chord': chord_counts.index[0],\n",
    "    }\n",
    "\n",
    "# Print summary\n",
    "for category, metrics in summary.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(\"-\" * 80)\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric:30s}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Insights and Recommendations\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Dataset Composition**\n",
    "   - The combined dataset provides a diverse collection of bass-focused musical material\n",
    "   - Train/validation/test splits are appropriately distributed for RL training\n",
    "\n",
    "2. **Musical Diversity**\n",
    "   - Wide BPM range supporting various tempo-based generation tasks\n",
    "   - Multiple genres represented, with cinematic and dance/jazz as primary categories\n",
    "   - Rich harmonic content with diverse chord progressions\n",
    "\n",
    "3. **Feature Characteristics**\n",
    "   - Predominantly 4/4 time signature (standard for most genres)\n",
    "   - Consistent measure lengths suitable for pattern learning\n",
    "   - Bass-focused pitch ranges appropriate for the generative task\n",
    "\n",
    "4. **Data Quality**\n",
    "   - Minimal missing values in critical features\n",
    "   - Consistent data formatting across both sources\n",
    "   - No significant duplicate issues\n",
    "\n",
    "### Recommendations for Multi-Agent RL Training:\n",
    "\n",
    "1. **GHSOM Pre-training**\n",
    "   - Use BPM, pitch range, and chord progression features for clustering\n",
    "   - Consider genre-specific clustering to capture style-specific motifs\n",
    "   - Pay attention to tempo-based groupings for rhythmic pattern discovery\n",
    "\n",
    "2. **DQN Agent Training**\n",
    "   - Balance rewards across tempo ranges to avoid BPM bias\n",
    "   - Use chord progression patterns as structural rewards\n",
    "   - Consider time signature as a constraint in action space\n",
    "\n",
    "3. **Human-in-the-Loop Adaptation**\n",
    "   - Focus feedback collection on genre-specific preferences\n",
    "   - Use BPM ranges to guide style-appropriate generation\n",
    "   - Leverage chord progression familiarity for quality assessment\n",
    "\n",
    "4. **Feature Engineering**\n",
    "   - Consider normalizing BPM to tempo categories (slow, medium, fast)\n",
    "   - Extract chord transition patterns for sequence modeling\n",
    "   - Encode time signatures for structural constraints\n",
    "\n",
    "### Potential Challenges:\n",
    "\n",
    "1. **Data Imbalance**\n",
    "   - Some genres are underrepresented\n",
    "   - Consider data augmentation or weighted sampling\n",
    "\n",
    "2. **Missing Metadata**\n",
    "   - Velocity information incomplete for some samples\n",
    "   - Audio key often unknown in bass_loops dataset\n",
    "   - May need imputation or fallback strategies\n",
    "\n",
    "3. **Complexity Range**\n",
    "   - Wide variation in measures and structure\n",
    "   - May need difficulty-based curriculum learning\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Perform GHSOM pre-training on selected features\n",
    "2. Extract and analyze discovered motifs\n",
    "3. Design reward functions based on feature distributions\n",
    "4. Implement data augmentation for underrepresented categories\n",
    "5. Set up monitoring for feature distribution shifts during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This comprehensive EDA has provided deep insights into the musical characteristics, structure, and quality of our training datasets. The analysis reveals a rich, diverse collection of bass-focused musical material suitable for training a multi-agent RL system for symbolic music generation.\n",
    "\n",
    "The datasets exhibit:\n",
    "- Strong genre diversity\n",
    "- Wide tempo and dynamic ranges\n",
    "- Rich harmonic content\n",
    "- Appropriate structural complexity\n",
    "\n",
    "These characteristics make them well-suited for training perceiving agents (GHSOM) to discover musical patterns and generative agents (DQN+LSTM) to create coherent musical sequences with human-in-the-loop guidance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
