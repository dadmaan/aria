# Final Baseline Configuration
# Incorporates all learnings from 7-phase benchmark campaign + extended training
# Baseline reward: 0.6530 (200K timesteps, +51.5% from campaign start)
# Created: 2025-12-06
# Updated: 2025-12-07 (Extended training winner)
# Updated: 2025-12-10 (Added new configurable parameters from agent_config.yaml)

# ========================================
# Phase 6 Winner: Prototype + t-SNE Observations
# ========================================
use_feature_observations: true
use_normalized_observations: false
feature_observation_mode: prototype
feature_observation_source: tsne

# ========================================
# Algorithm Configuration
# ========================================
algorithm:
  type: dqn
  is_double: true
  distributional:
    num_atoms: 51
    v_min: -3.0
    v_max: 7.0
  prioritized_replay:
    enabled: false
    alpha: 0.6
    beta: 0.4
    beta_final: 1.0
    beta_anneal_step: null

# ========================================
# Replay Buffer Configuration (R2D2-style)
# ========================================
replay_buffer:
  stack_num: 1
  sequence:
    enabled: true
    sequence_length: 16
    min_sequence_length: 4
    store_hidden_states: true
    respect_episode_boundaries: true
  burn_in:
    enabled: true
    burn_in_length: 8
    use_stored_hidden: true

# ========================================
# Phase 1 Winner: LSTM Architecture (h=256)
# ========================================
network:
  type: drqn
  embedding_dim: 64
  fc_hidden_sizes:
    - 128
    - 64
  activation_fn: elu
  dropout: 0.2
  lstm:
    hidden_size: 256
    num_layers: 1
  diagnostics:
    enabled: true
    log_interval: 1000

# ========================================
# Training Configuration
# Phase 2: Learning Rate = 0.0001
# Phase 3: Exponential epsilon schedule
# Phase 4: Fast epsilon decay (30k steps)
# Phase 7: Pulse mechanism DISABLED
# ========================================
training:
  total_timesteps: 200000  # Extended training winner: +1.4% gain vs 100K
  step_per_epoch: 1000
  step_per_collect: 1
  episode_per_test: 10
  batch_size: 32
  buffer_size: 10000
  learning_rate: 0.0001
  gamma: 0.95
  target_update_freq: 1000
  log_interval: 100
  n_step_return_horizon: 1  # Standard TD(0), higher values enable multi-step returns

  # Phase 3 & 4: Fast exponential epsilon decay
  exploration:
    initial_eps: 1.0
    final_eps: 0.05
    fraction: 0.3
    noise: true  # Enable exploration noise during collection

  # Phase 7 Winner: Pulse mechanism DISABLED
  learning_rate_scheduler:
    enabled: true
    type: exponential
    initial_lr: 0.001
    final_lr: 0.0001
    decay_rate: 0.995
    decay_steps: 5000
    pulse_mechanism:
      enabled: false
      duration_episodes: 50
      boost_epsilon: 0.3
      trigger_mode: adaptive
      trigger_threshold: 8.0
      adaptive_threshold:
        enabled: true
        baseline_alpha: 0.1
        relative_drop_threshold: 0.15

  start_timesteps: 1000

# ========================================
# Phase 5 Winner: Transition-Heavy Reward Weights
# Structure: 0.25, Transition: 0.50, Diversity: 0.25
# ========================================
use_incremental_rewards: true
reward_context_window: 5

reward_components:
  structure:
    weight: 0.25
    enabled: true

  transition:
    weight: 0.5
    enabled: true
    structural_weight: 0.3
    max_distance: 60.0

  diversity:
    weight: 0.25
    enabled: true
    optimal_ratio_low: 0.62
    optimal_ratio_high: 0.75
    repetition_penalty: -0.3

feature_weights:
  distance_weight: 1.0
  neighbor_weight: 0.5
  grid_weight: 0.25

# ========================================
# Environment Configuration
# ========================================
music:
  sequence_length: 16

ghsom:
  checkpoint: null
  default_model_path: artifacts/ghsom_commu_tsne/ghsom_model.pkl

features:
  artifact_path: artifacts/features/tsne/commu_full_filtered_tsne
  type: tsne

# ========================================
# Logging Configuration
# ========================================
logging:
  wandb:
    enabled: true
    project: ARIA_rl_V04
    entity: null
    tags: []
    log_interval: 10
    save_code: true

  tensorboard:
    enabled: true
    log_dir: logs
    log_interval: 10
    log_histograms: false
    histogram_interval: 100

  comprehensive:
    enabled: true
    log_system_metrics: true
    log_model_info: true
    log_ghsom_metrics: true
    log_training_metrics: true
    log_to_tensorboard: true
    log_to_local: true
    system_metrics_freq: 10
    q_value_log_freq: 10
    gradient_log_freq: 10

  reward_components:
    enabled: true
    log_frequency: 10
    export_frequency: 1000
    log_to_wandb: true
    log_to_tensorboard: true
    log_to_local: true
    max_history: 10000

paths:
  output_dir: artifacts/training
  tensorboard_log_dir: logs
  checkpoint_dir: null

evaluation:
  enabled: true
  interval: 100
  num_episodes: 10
  deterministic: true

# ========================================
# System Configuration
# ========================================
enable_terminal_ui: true
terminal_ui:
  mode: simple
  refresh_rate: 4
  show_sparklines: true
  show_sequence_visual: true
  show_ghsom_hierarchy: true
  max_history_length: 100

system:
  device: auto
  enable_gpu: true
  seed: 42
  verbose: 1

monitoring:
  log_training_metrics: true
  q_value_log_freq: 100
  gradient_log_freq: 100

interaction_timeout: 30
non_interactive_mode: true
human_feedback_enabled: false
human_feedback_timeout: 5
cli_human_feedback: null
render_midi: false
print_episode_output: false

debug:
  summarize_grads_and_vars: false
  debug_summaries: false

# ========================================
# Checkpointing Configuration
# ========================================
checkpointing:
  save_freq: 10000
  save_best: true
  keep_last_n: 5

# ========================================
# Curriculum Learning (disabled for baseline)
# ========================================
curriculum:
  enabled: false
  ghsom_experiment_path: null
  timesteps_per_action: 2500
  patience_per_action: 150
  plateau_threshold: 0.01
  transition:
    flush_buffer: true
    epsilon_boost: 0.3
    add_mitosis_noise: true
    mitosis_noise_scale: 0.01

# ========================================
# Campaign Results Summary
# ========================================
# Phase 1 (Architecture): h=256 → 0.5493
# Phase 2 (Learning Rate): lr=0.0001 → 0.5651 (+2.88%)
# Phase 3 (Epsilon): exponential → 0.5651 (validated)
# Phase 4 (Decay): fast 30k → 0.6311 (+11.68%)
# Phase 5 (Rewards): 0.25/0.50/0.25 → 0.6324 (+0.21%)
# Phase 6 (Observation): Prototype+tSNE → 0.6330 (+0.09%)
# Phase 7 (Pulse): OFF → 0.6440 (+1.74%)
# Extended Training (200K): → 0.6530 (+1.40%)
#
# TOTAL IMPROVEMENT: +51.5% (0.4310 → 0.6530)
# ========================================
